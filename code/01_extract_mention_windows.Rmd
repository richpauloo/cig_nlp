---
title: "Extract Mention Windows"
output: html_document
---


The purpose of this script is to read in all PDFs from 2010-2015 and extract the senteces matching CIG software, with a window of one sentence before and after the software mention.

Packages.
```{r}
library(pdftools)
library(tokenizers)
library(stringr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(viridis)
library(colormap)
```


Vector of package names.
```{r}
cig <- c("ASPECT ", "Aspect ",
"AxiSEM", "axisem", "AXISEM",
"Burnman", "burnman",
"Calypso",
"Citcom", "citcomcu", "citcomS", "citcoms", "CitcomS", "CITCOM-CU", "CITCOM", "CitComS",
"ConMan", "conman",
"Ellipsis3d", "ellipsis", "ellipsis3d", "Ellipsis",
"FLEXWIN", "flexwin",
" Gale ",    # avoid "Galerkin"
" MAG ",     # avoid picking up "magnetic", "magnitude", etc..
"Mineos", "mineos", "MINEOS",
"PyLith", "pylith", "Pylith",
# "Rayleigh", # too many false positives. will need to do manually later
"RELAX",
"SEISMIC_CPML", "seismic cpml", 
"SELEN", "selen ",
"SNAC",
"SPECFEM", "specfem2d", "specfem3d_globe", "specfem3d_cartesian", "Specfem3-D Globe", "specter", "Specfem2d",
"SW4", "sw4",
"Virtual California",
"Gmsh", 
"GMT ", "Generic Mapping Tool", "GenericMappingTool",
# new additions as of 2019-04-10
"Obspy", "ObsPy",
"Instaseis", "instaseis",
"GPlates", "Gplate", "GPlate",
"gmsh", 
"MATLAB",
" SAC ",
"Zenodo",
" CIG ", "(CIG)", "CIG-II")

#write_tsv(x = data.frame(term = cig), path = "C:/Users/rpauloo/Desktop/whitelist.tsv")
```


```{r}
fp <- "F:/Box Sync/2019 CItation/Papers/" # PC
#fp <- "/Users/richpauloo/Desktop/2019 CItation/Papers/" # Mac

# paper names
p <- list.files(paste0(fp, "all_papers")) 

# extract numeric index
n_index <- str_split(p, " ") %>% 
  sapply(., function(x){x[[1]]}) %>% 
  str_sub(., 1, 4) %>% 
  str_extract_all(., "[:digit:]") %>% 
  sapply(., function(x){paste(x, collapse = "")}) %>% 
  as.numeric()

# make file-journal key
# library(gtools); write_csv(data.frame(file = mixedsort(p)), path = "/Users/richpauloo/Documents/GitHub/cig_nlp/rich_data/paper_journal_key_2.csv")
```

For one PDF, get vector of paper file names, read in the first one, tokenize sentences, and extract the mention window.
```{r}
# # pdf_info(paste0(fp, "all_papers/", p[1])) # metadata: can grab all DOI
# 
# # read all pdfs
# raw <- paste(pdf_text(paste0(fp, "all_papers/", p[1])), collapse = " ")
#   
# # tokensize sentence with tokenizers package
# rs <- tokenize_sentences(raw[1])                      # tokenize sentences
# rs <- lapply(rs, function(x){paste(x, collapse = " ")}) # collapse each list element
# 
# # do for one
# x = rs[[1]]
# 
# # grabs mention window if a software (y) is mentioned
# match_words <- function(y){
#   hold <- NA
#   i    <- str_which(x, y)
#   if(!identical(i, integer(0))){
#     hold <- paste(x[i-2], x[i-1], x[i], x[i+1], x[i+2])
#   }
#   # hold <- data.frame(match = y, matches = hold)
#   return(hold)
# }
# 
# # applies vector of software names to `match_words` to get list of all windows
# matches <- lapply(cig, match_words)
# 
# # name the list by software name, filter out NA, and organize into a dataframe
# names(matches) <- cig
# matches <- matches[!is.na(matches)] 
# data.frame(matches) %>% 
#   gather(software_name, value) %>% 
#   distinct()
```

Function to extract mention windows to all PDFs.
```{r}
# function to read PDFs
read_pdfs <- function(p){
  paste(pdf_text(paste0(fp, "all_papers/", p)), collapse = " ")
}

read_pdfs_pages <- function(p){
  pdf_info(paste0(fp, "all_papers/", p))$pages
}

# read all pdfs into a list: takes a while
#raw <- lapply(p, read_pdfs)

#read in tryCatch to handle errors
# raw <- vector("list", length = length(p))
# for(i in 1:length(p)){
#     raw[[i]] <- tryCatch(
#       {
#         print(paste(i, "Now reading:", p[i]))
#         paste(pdf_text(paste0(fp, "all_papers/", p[i])), collapse = " ")
#       },
#       error = function(cond){
#         message(paste(i, "Error reading:", p[i]))
#         message(cond)
#         return(p[i])
#       },
#       function(cond){
#         message(paste(i, "Warning reading:", p[i]))
#         message(cond)
#         return(p[i])
#       },
#       finally = {message(paste("Successfully read:", p[i]))}
#   )
# }

# # get npages for each document.
# n_pages <- vector(length = length(p))
# for(i in 1:length(p)){
#     n_pages[[i]] <- tryCatch(
#       {
#         print(paste(i, "Now reading:", p[i]))
#         pdf_info(paste0(fp, "all_papers/", p[i]))$pages
#       },
#       error = function(cond){
#         message(paste(i, "Error reading:", p[i]))
#         message(cond)
#         return(p[i])
#       },
#       function(cond){
#         message(paste(i, "Warning reading:", p[i]))
#         message(cond)
#         return(p[i])
#       },
#       finally = {message(paste("Successfully read:", p[i]))}
#   )
# }

# fix errors
#n_pages[c(152,314,395)] <- c(11,34,47)

#n_pages <- data.frame(p = p, n_pages = n_pages)
#readr::write_rds(n_pages, "C:/Users/rpauloo/Documents/Github/cig_nlp/rich_data/n_pages.rds") # save the data
#readr::write_rds(raw, "C:/Users/rpauloo/Documents/Github/cig_nlp/rich_data/raw2.rds") # save the data
raw <- read_rds("C:/Users/rpauloo/Documents/Github/cig_nlp/rich_data/raw2.rds")

# tokenize sentences
rs <- tokenize_sentences(raw)

# find mention windows for all words in all PDFs

# function to match words and return mention windows within one PDF
match_words <- function(y){
  
  # intialize return variable
  hold <- NA
  
  # get index of all word matches
  i    <- str_which(x, y)  
  
  # if there's a match
  if(!identical(i, integer(0))){ 
    # case 1: match at first sentence
    if(1 %in% i){
      hold <- paste(x[i], x[i+1], x[i+2])
    }
    # case 2: match at second sentence
    else if(2 %in% i){
      hold <- paste(x[i-1], x[i], x[i+1], x[i+2])
    }
    # case 3: match at last sentence
    else if(length(x) %in% i){
      hold <- paste(x[i-2], x[i-1], x[i])
    }
    # case 4: match at second to last sentence
    else if((length(x) - 1) %in% i){
      hold <- paste(x[i-2], x[i-1], x[i], x[i+1])
    }
    # case 5: match NOT at 1st, 2nd, 2nd to last, or last sentence
    else{
      hold <- paste(x[i-2], x[i-1], x[i], x[i+1], x[i+2])
    }
  }
  return(hold) # original working
}

# match references
temp    <- lapply(rs, str_which, "[Acknowledgments | ACKNOWLEDGMENTS]")
ref_pos <- sapply(temp, function(x){last(x)})
```

Extract mention windows, and record: software name match, paper title. Store in a data frame.
```{r}
# initalize list that will store all clean data frames
df <- vector("list", length = length(raw)) # testing with one paper

# loop over all PDFs, and grab mention windows
for(i in 1:length(raw)){
  x = rs[[i]]                          # select PDF text
  matches <- lapply(cig, match_words)  # find matches
  names(matches) <- cig                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)

  nam <- names(matches)                # retreive names of remaining software
  
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)})
  
  # if there is a match
  if(length(temp) > 0){
    # get the software name for each df
    for(j in 1:length(temp)){         
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  # if there is no match
  if(length(temp) == 0){
    df[[i]]$software_name <- NA
    df[[i]]$value         <- NA
    df[[i]]$paper         <- p[i]
  }
}

# combine into one final data frame
df1 <- do.call(rbind.data.frame, df)

# recombine into main group names
df1 <- mutate(df1, software_name = case_when(software_name %in% c("axisem", "AXISEM") ~ "AxiSEM",
                                             software_name == "burnman" ~ "Burnman",
                                             software_name %in% c("citcomcu", "citcomS", "citcoms", "CitcomS", "CITCOM-CU", "CITCOM", "CitComS") ~ "Citcom",
                                             software_name == "conman" ~ "ConMan",
                                             software_name %in% c("ellipsis", "ellipsis3d", "Ellipsis") ~ "Ellipsis3d",
                                             software_name %in% c("mineos", "MINEOS") ~ "Mineos",
                                             software_name == "flexwin" ~ "FLEXWIN",
                                             software_name %in% c("pylith", "Pylith") ~ "PyLith",
                                             software_name == "seismic cpml" ~ "SEISMIC_CPML",
                                             software_name %in% c("specfem2d", "specfem3d_globe", "specfem3d_cartesian", "Specfem3-D Globe", "specter", "Specfem2d") ~ "SPECFEM",
                                             software_name == "sw4" ~ "SW4",
                                             software_name == "selen " ~ "SELEN",
                                             software_name %in% c("Aspect ", "ASPECT ") ~ "ASPECT",
                                             software_name %in% c("GMT ","Generic Mapping Tool", "GenericMappingTool") ~ "GMT",
                                             software_name == "Obspy" ~ "ObsPy",
                                             software_name == "instaseis" ~"Instaseis",
                                             software_name %in% c("GPlates", "Gplate") ~ "GPlate",
                                             software_name %in% c("Gmsh","gmsh") ~ "GMSH",
                                             software_name %in% c(" CIG ","(CIG)", "CIG-II") ~ "CIG",
                                             TRUE ~ software_name))
# count of unique mentions in all docs
df1 %>% count(software_name) %>% arrange(desc(n)) 

# count of mentions per unique papers
df1 %>% count(software_name, paper) %>% arrange(desc(n)) 

# number of papers that don't mention one of the softwares.
nrow(filter(df1, is.na(software_name)))

# names of papers missing a software package
pull(filter(df1, is.na(software_name)), paper)
```

Determine co-mentions.
```{r}
n_co_mentions <- df1 %>% 
  group_by(paper) %>% 
  summarise(n_co_mentions = length(unique(software_name)))

# grab co-mentions per paper
l <- split(df1, df1$paper)
l <- sapply(l, function(x){paste(unique(x$software_name), collapse = ", ")})
l <- data.frame(paper = names(l), co_mentioned_software = l)
rownames(l) <- NULL

# merge
df2 <- left_join(n_co_mentions, l, by = "paper") %>% filter(n_co_mentions >= 2) 
```


Write data.
```{r}
library(xlsx)
write.xlsx(df1, "C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_data/mention_windows.xlsx", row.names = FALSE)
write.xlsx(data.frame(paper = pull(filter(df1, is.na(software_name)), paper)), 
          "C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_data/missing_software.xlsx", row.names = FALSE)
write.xlsx(as.data.frame(df2), "C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_data/co_mention_summary.xlsx", row.names = FALSE)
```


Now extract sentence token *index* at which software mention occurs, and the sentence token *length* per paper. The ratio of: $\frac{token \space index}{ token \space length}$ is an indication of where in the paper the mention occurs. In the future, find last mention of References so we know if a mention occurs in the references. But as a first pass, this is okay. 

```{r}
# initalize list that will store all clean data frames
df3 <- vector("list", length = length(raw)) # testing with one paper

# loop over all PDFs, and grab token index, token length
for(i in 1:length(raw)){
  x = rs[[i]]                          # select PDF text
  token_length = length(rs[[i]])       # total number of tokens in paper
  indices <- lapply(cig, function(y){return(str_which(x, y))})
  names(indices) <- cig                # name list element by software
  indices <- indices[!sapply(indices, identical, integer(0))]  # remove non-matches (integer(0))
  
  nam <- names(indices)                # retreive names of remaining software
  
  temp <- lapply(indices, function(z){ # gather each list into a dataframe
    return(data.frame(token_index = z))})
  
  # if there is a match
  if(length(temp) > 0){
    temp <- bind_rows(temp)              # bind into a dataframe
    temp$token_length <- token_length    # cbind the total token length of the paper
    df3[[i]] <- temp                     # store in final df
  }
  
  # if there is no match
  if(length(temp) == 0){
    temp <- data.frame(token_index = NA, token_length = token_length)
    df3[[i]] <- temp
  }
}

# combine into one final data frame
df3 <- do.call(rbind.data.frame, df3)

# combine with previous dataframe of mention windows, software namaes, paper names
final <- cbind.data.frame(df1, df3)

# calculate (token_index / token_length) = mention position in paper
final <- mutate(final, token_position = token_index / token_length)

# calculate error
n_pages <- readr::read_rds("C:/Users/rpauloo/Documents/Github/cig_nlp/rich_data/n_pages.rds")
final   <- left_join(final, n_pages, by = c("paper" = "p"))
final <- final %>% 
    mutate(n_pages = as.numeric(as.character(n_pages)), 
           tokens_per_page = token_length/n_pages, 
           token_error = tokens_per_page/2, 
           error_low = token_index - token_error, 
           error_high = token_index + token_error, 
           error_low = ifelse(error_low <= 0, 0, error_low), 
           error_high = ifelse(error_high >= token_length, token_length, error_high),
           error_low_p = error_low/token_length,
           error_high_p = error_high/token_length)

# add journal alias
n_index_df <- data.frame(alias = n_index, p = p)
final <- left_join(final, n_index_df, by = c("paper" = "p"))

# add median token position
med_token_pos <- group_by(final, software_name) %>% 
  summarise(med_token_pos = median(token_position, na.rm=T))
final <- left_join(final, med_token_pos, by = "software_name")
```


Determine if a mention occurs in the References section. Not straightforward to simply extract "references" because section headers are often not 
```{r}
# rsl <- lapply(rs, tolower) # lowercase
# temp <- lapply(rsl, str_which, "references")
# temp <- lapply(rs, str_which, "References")
```


Sanity check. Also, calculate total mentions and total unique mentions.
```{r}
# total mentions
total_mentions <- final %>% 
  filter(!is.na(software_name) & software_name != "Rayleigh") %>% 
  nrow()

# unique mentions per paper
unique_mentions <- final %>% 
  group_by(software_name) %>% 
  summarise(n_papers = n_distinct(paper)) %>% 
  filter(!is.na(software_name)) %>% 
  arrange(desc(n_papers)) %>% 
  filter(software_name != "Rayleigh") %>% 
  pull(n_papers) %>% 
  sum()

# Lorraine was getting 279 total mentions, and I get 243. This automated pipeline works.
```


Replace token position with nchar position
```{r}
nchar_df <- data.frame(p = p, 
                       nchar = sapply(rs, function(x){sum(nchar(x))}),
                       ref_pos = ref_pos)
final <- left_join(final, nchar_df, by = c("paper" = "p"))

temp  <- split(final, final$paper)
temp2 <- vector("list", length = length(rs))
temp3 <- vector("list", length = length(rs))

for(i in 1:length(rs)){
  chars <- nchar(rs[[i]])
  ve    <- vector(length = length(temp[[i]]$token_index))
  ve2   <- vector(length = length(temp[[i]]$token_index))
  if(!is.na(temp[[i]]$software_name)){
    for(j in 1:length(temp[[i]]$token_index)){
      ve[[j]]  <- sum(chars[1:temp[[i]]$token_index[j]], na.rm = TRUE)
      ve2[[j]] <- sum(chars[1:temp[[i]]$ref_pos[j]], na.rm = TRUE)
    }
    temp2[[i]] <- ve
    temp3[[i]] <- ve2
  }
  if(is.na(temp[[i]]$software_name)){
    temp2[[i]] <- NA
    temp3[[i]] <- NA
  }
}

# add error and ref pos
final$token_nchar <- do.call(c, temp2)
final$ref_nchar   <- do.call(c, temp3)
final <- mutate(final, 
                nchar_token_position = token_nchar/nchar,
                nchar_ref_position = ref_nchar/nchar)

final <- final %>% 
    mutate(nchar_per_page = nchar/n_pages, 
           nchar_error = nchar_per_page/2, 
           nchar_error_low = token_nchar - nchar_error, 
           nchar_error_high = token_nchar + nchar_error, 
           nchar_error_low = ifelse(nchar_error_low <= 0, 0, nchar_error_low), 
           nchar_error_high = ifelse(nchar_error_high >= nchar, nchar, nchar_error_high),
           nchar_error_low_p = nchar_error_low/nchar,
           nchar_error_high_p = nchar_error_high/nchar,
           ref_nchar = ref_nchar - nchar_error*2,
           is_ref = ifelse(ref_nchar <= token_nchar, TRUE, FALSE))

# add median token position
nchar_med_token_pos <- group_by(final, software_name) %>% 
  summarise(nchar_med_token_pos = median(nchar_token_position, na.rm=T))
final <- left_join(final, nchar_med_token_pos, by = "software_name")

# plot
sw <- unique(final$software_name)
sw <- sw[!is.na(sw)]
med_token_pos_df <- distinct(final, software_name, nchar_med_token_pos) %>%
  filter(!is.na(nchar_med_token_pos))

p_list <- vector("list", length = length(sw))

for(i in 1:length(p_list)){
  p_list[[i]] <- final %>% 
    filter(software_name == sw[i]) %>% 
    ggplot(aes(x = forcats::fct_reorder(factor(alias), nchar_token_position))) +
    geom_segment(aes(y = nchar_error_low_p, 
                     yend = nchar_error_high_p, 
                     xend = forcats::fct_reorder(factor(alias), nchar_token_position)), color = "red") +
    geom_point(aes(y = nchar_token_position, color = is_ref), size = 2) +
    geom_hline(yintercept = med_token_pos_df[i, 2], linetype = "dashed") +
    coord_flip(ylim = c(0,1)) + 
    theme(panel.background = element_rect(fill = "white"),
          panel.grid.major.y = element_line(color = "grey90"),
          panel.grid.major.x = element_blank()) + 
    scale_color_manual(breaks = c(TRUE,FALSE), 
                       values = c("black","purple")) +
    labs(x = "", y = "Position in text", 
         color = "Acknowledgement or Reference",
         title = sw[i],
         subtitle = paste0("Median token position = ", round(med_token_pos_df[i, 2],2))) 
}

# Creating new PDF file
pdf("C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_plots/nchar_software_mentions.pdf", width = 11 , height = 11)
 
for(i in 1:length(p_list)){
   print(p_list[[i]])
}
 
dev.off()

```



<!-- Make Lorraine's graph. Order y axis paper aliases by median token_position. -->
<!-- ```{r} -->
<!-- sw <- unique(final$software_name) -->
<!-- sw <- sw[!is.na(sw)] -->
<!-- p_list <- vector("list", length = length(sw)) -->
<!-- med_token_pos_df <- distinct(final, software_name, med_token_pos) %>% -->
<!--   filter(!is.na(med_token_pos)) -->

<!-- for(i in 1:length(p_list)){ -->
<!--   p_list[[i]] <- final %>% -->
<!--     filter(software_name == sw[i]) %>% -->
<!--     ggplot(aes(x = forcats::fct_reorder(factor(alias), token_position))) + -->
<!--     geom_segment(aes(y = error_low_p, -->
<!--                      yend = error_high_p, -->
<!--                      xend = forcats::fct_reorder(factor(alias), token_position)), color = "red") + -->
<!--     geom_point(aes(y = token_position), color = "black", size = 2) + -->
<!--     geom_hline(yintercept = med_token_pos_df[i, 2], linetype = "dashed") + -->
<!--     coord_flip(ylim = c(0,1)) + -->
<!--     theme(panel.background = element_rect(fill = "white"), -->
<!--           panel.grid.major.y = element_line(color = "grey90"), -->
<!--           panel.grid.major.x = element_blank()) + -->
<!--     guides(color = FALSE) + -->
<!--     labs(x = "", y = "Position in text", -->
<!--          title = sw[i], -->
<!--          subtitle = paste0("Median token position = ", round(med_token_pos_df[i, 2],2))) -->
<!-- } -->

<!-- # Creating new PDF file -->
<!-- pdf("C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_plots/software_mentions.pdf", width = 11 , height = 11) -->

<!-- for(i in 1:length(p_list)){ -->
<!--    print(p_list[[i]]) -->
<!-- } -->

<!-- dev.off() -->
<!-- ``` -->


Visualize distribution of position occurence.
```{r}
# library(ggridges)
# final %>% 
#   filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ") ) %>% 
#   ggplot(aes(token_position, software_name, fill = ..x..)) + 
#   geom_density_ridges_gradient(scale = 1.5, rel_min_height = 0.01) + 
#   scale_fill_viridis(option = "magma")

med_vec <- final %>% 
  # filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ", "Rayleigh") ) %>%
  filter(!is.na(software_name) & !software_name %in% c("Rayleigh") ) %>%
  group_by(software_name) %>% 
  summarise(med = median(nchar_token_position)) %>% 
  arrange(-med) %>% pull(software_name)

# define color pallette
cols <- colormap(colormap = colormaps$viridis, nshades = length(med_vec))
cols <- structure(cols, names = med_vec)

library(glue)
p1 <- final %>% 
  filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ", "Rayleigh", "SEISMIC_CPML") ) %>%
  filter(!is.na(software_name) & !software_name %in% c("Rayleigh") ) %>%
  ggplot(aes(x=reorder(software_name,nchar_token_position, FUN = median), nchar_token_position, fill = software_name)) + 
  geom_boxplot(alpha = 0.5) + 
  #geom_jitter(width = 0.2, alpha = 0.1) + 
  coord_flip() + 
  theme_minimal(base_size = 15) + 
  guides(fill = FALSE) +
  scale_fill_manual(values = cols,
                    breaks = med_vec) +
  labs(title = "CIG software mentions (2010-2018)", 
       subtitle = glue("Total mentions (n = {total_mentions}); Unique mentions (n = {unique_mentions})"),
       x = "", y = "Location of Mention in Document", 
       caption = "SEISMIC_CPML, SW4, SNAC, MAG, Rayleigh removed")

p1

# save
#ggsave(p, filename = "cig_total_and_unique_mentions_all.pdf", device = cairo_pdf, width = 11, height = 8)
ggsave(p1, filename = "C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_plots/nchar_cig_total_and_unique_mentions_trunc.pdf", device = cairo_pdf, width = 11, height = 8)

#getwd()
```


Turn co-occurences into plot.
```{r}
co <- df2 %>% select(-paper) %>% rename(n = n_co_mentions, x = co_mentioned_software)

library(tidyverse)
library(combinat)
cor <- co %>% 
    ## Parse entries in x into distinct elements
    mutate(split = map(x, str_split, pattern = ', '), 
           flat = flatten(split)) %>% 
    ## Construct 2-element subsets of each set of elements
    mutate(combn = map(flat, combn, 2, simplify = FALSE)) %>% 
    unnest(combn) %>% 
    ## Construct permutations of the 2-element subsets
    mutate(perm = map(combn, permn)) %>% 
    unnest(perm) %>% 
    ## Parse the permutations into row and column indices
    mutate(row = map_chr(perm, 1), 
           col = map_chr(perm, 2)) %>% 
    count(row, col) %>% 
    ## Long to wide representation
    spread(key = col, value = n, fill = 0) %>% 
    ## Coerce to matrix
    column_to_rownames(var = 'row') %>% 
    as.matrix()

library(reshape2)
cor[upper.tri(cor, diag=TRUE)] <- NA # remove upper triangle
melt(cor) %>% ggplot(aes(Var1, Var2, fill = value)) + geom_raster()

co_plot <- melt(cor) %>% 
    filter(!is.na(value)) %>% 
    mutate(value=ifelse(Var1=="RELAX"&Var2=="ASPECT", 0, value),
           value = ifelse(Var1=="RELAX"&Var2=="Burnman", 0, value),
           Var1 = as.character(Var1), Var2 = as.character(Var2),
           Var1 = ifelse(Var1 == "Virtual California", "Virtual CA", Var1),
           Var2 = ifelse(Var2 == "Virtual California", "Virtual CA", Var2)) %>% 
    ggplot(aes(Var1, Var2, fill = factor(value))) + 
    geom_tile(color = "white") + 
    geom_text(aes(label = value), color = "white") +
    scale_fill_manual(values = c("grey50", 
                                 colormap(colormaps$viridis, 
                                          nshades = 22))) +
    theme_minimal(base_size = 20) +
    theme(legend.position = "bottom") + 
    guides(fill = guide_legend(nrow = 2, byrow = TRUE)) +
    labs(fill = "Co-occurences", x = "", y = "") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
co_plot

ggsave(co_plot, filename = "C:/Users/rpauloo/Documents/GitHub/cig_nlp/rich_plots/co_occurrence_plot.pdf", device = cairo_pdf, height = 9, width = 16)
```


Make adjacency matrix of software packages, and distance matrix. Make into graph.
```{r}

```


Clever way of determining if something is a reference or not. Can be rule-based. Look at mention window. Do the adjacent entries contain "referencey" items like "A."? Replot `p`, but make the references mentions another color.
```{r}

```

























```{r}
myfun <- function(y){
  
  # intialize return variable
  hold <- NA
  
  # get index of all word matches
  i <- str_which(x, y)  

  return(hold) # original working
  return(data.frame(sen = hold, i = i)) # adding sentence index to hold vector
}

df <- list()

for(i in 1:1){
  x = rs[[i]]                          # select PDF text
  matches <- lapply(cig, myfun)        # find matches
  names(matches) <- cig                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)
  
  nam <- names(matches)                # retreive names of remaining software
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)}) 
  
  if(length(temp) > 0){
    for(j in 1:length(matches)){         # fill in the software name for each df
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  if(length(temp) == 0){
    df[[i]] <- NA
  }
}



```



Combine with Duncan's ReadPDF
```{r}
# first paper's names(ReadPDF::getSectionText(p[1])), where p[1] is XML
zn <- c("", "Introduction", "", "Numerical Model of Fault Slip", "", 
"Finite-Element Mesh Processing", "", "Solver Customization", 
"", "Performance Benchmark", "", "Code Verification Benchmarks", 
"", "Conclusions", "", "Notation", "References", "<other>", "Table1", 
"Table2", "Table3", "Table4", "Table5", "Table6", "Table7", "Table8"
)

# remove artifacts from ReadPDF
zn <- zn[! zn %in% c("", "<other>") ] 
z <- zn[-str_which(zn, "Table")]      # vector of terms to search


# look for indices of z in pdf_text of p[1]
raw[[1]] # p[1]
rs[[1]]  # p[1]

```



Repeat with lowercase.
```{r}
rsl  <- lapply(rs, tolower)
cigl <- tolower(cig)


# intalize list that will store all clean data frames
df <- vector("list", length = length(p)) 

# loop over all PDFs, and grab mention windows
library(tidyr)
for(i in 1:length(p)){
  x = rsl[[i]]                          # select PDF text
  matches <- lapply(cigl, match_words)  # find matches
  names(matches) <- cigl                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)
  
  nam <- names(matches)                # retreive names of remaining software
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)}) 
  
  if(length(temp) > 0){
    for(j in 1:length(matches)){         # fill in the software name for each df
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  if(length(temp) == 0){
    df[[i]] <- NA
  }
}

# combine into one final data frame
final <- do.call(rbind.data.frame, df)
```


```{r}
final %>% 
  group_by(software_name) %>% 
  summarise(n_papers = n_distinct(paper)) %>% 
  filter(!is.na(software_name)) %>% 
  arrange(desc(n_papers)) %>% 
  filter(software_name != "rayleigh") %>% 
  pull(n_papers) %>% 
  sum()
```


