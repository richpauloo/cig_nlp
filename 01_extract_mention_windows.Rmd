---
title: "Extract Mention Windows"
output: html_document
---


The purpose of this script is to read in all PDFs from 2010-2015 and extract the senteces matching CIG software, with a window of one sentence before and after the software mention.

Packages.
```{r}
library(pdftools)
library(tokenizers)
library(stringr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(viridis)
library(colormap)
```


Vector of package names.
```{r}
cig <- c("ASPECT",
"AxiSEM",
"Burnman",
"Calypso",
"Citcom",
"ConMan",
"Ellipsis3d",
"FLEXWIN",
" Gale ",    # avoid "Galerkin"
" MAG ",     # avoid picking up "magnetic", "magnitude", etc..
"Mineos",
"PyLith",
"Rayleigh",
"RELAX",
"SEISMIC_CPML",
"SELEN",
"SNAC",
"SPECFEM",
"SW4",
"Virtual California")
```


```{r}
fp <- "C:/Users/rpauloo/Desktop/2019 CItation/Papers/" # PC
# fp <- "/Users/richpauloo/Desktop/2019 CItation/Papers/" # Mac

# paper names
p <- list.files(paste0(fp, "all_papers")) 
```

For one PDF, get vector of paper file names, read in the first one, tokenize sentences, and extract the mention window.
```{r}
# # pdf_info(paste0(fp, "all_papers/", p[1])) # metadata: can grab all DOI
# 
# # read all pdfs
# raw <- paste(pdf_text(paste0(fp, "all_papers/", p[1])), collapse = " ")
#   
# # tokensize sentence with tokenizers package
# rs <- tokenize_sentences(raw[1])                      # tokenize sentences
# rs <- lapply(rs, function(x){paste(x, collapse = " ")}) # collapse each list element
# 
# # do for one
# x = rs[[1]]
# 
# # grabs mention window if a software (y) is mentioned
# match_words <- function(y){
#   hold <- NA
#   i    <- str_which(x, y)
#   if(!identical(i, integer(0))){
#     hold <- paste(x[i-2], x[i-1], x[i], x[i+1], x[i+2])
#   }
#   # hold <- data.frame(match = y, matches = hold)
#   return(hold)
# }
# 
# # applies vector of software names to `match_words` to get list of all windows
# matches <- lapply(cig, match_words)
# 
# # name the list by software name, filter out NA, and organize into a dataframe
# names(matches) <- cig
# matches <- matches[!is.na(matches)] 
# data.frame(matches) %>% 
#   gather(software_name, value) %>% 
#   distinct()
```

Function to extract mention windows to all PDFs.
```{r}
# function to read PDFs
read_pdfs <- function(p){
  paste(pdf_text(paste0(fp, "all_papers/", p)), collapse = " ")
}

# read all pdfs into a list: takes a while
# raw <- lapply(p, read_pdfs)
# readr::write_rds(raw, "raw.rds") # save the data
raw <- read_rds("raw.rds")

# tokenize sentences
rs <- tokenize_sentences(raw)

# find mention windows for all words in all PDFs

# function to match words and return mention windows within one PDF
match_words <- function(y){
  
  # intialize return variable
  hold <- NA
  
  # get index of all word matches
  i    <- str_which(x, y)  
  
  # if there's a match
  if(!identical(i, integer(0))){ 
    # case 1: match at first sentence
    if(1 %in% i){
      hold <- paste(x[i], x[i+1], x[i+2])
    }
    # case 2: match at second sentence
    else if(2 %in% i){
      hold <- paste(x[i-1], x[i], x[i+1], x[i+2])
    }
    # case 3: match at last sentence
    else if(length(x) %in% i){
      hold <- paste(x[i-2], x[i-1], x[i])
    }
    # case 4: match at second to last sentence
    else if((length(x) - 1) %in% i){
      hold <- paste(x[i-2], x[i-1], x[i], x[i+1])
    }
    # case 5: match NOT at 1st, 2nd, 2nd to last, or last sentence
    else{
      hold <- paste(x[i-2], x[i-1], x[i], x[i+1], x[i+2])
    }
  }
  return(hold) # original working
}
```

Extract mention windows, and record: software name match, paper title. Store in a data frame.
```{r}
# initalize list that will store all clean data frames
df <- vector("list", length = length(raw)) # testing with one paper

# loop over all PDFs, and grab mention windows
for(i in 1:length(raw)){
  x = rs[[i]]                          # select PDF text
  matches <- lapply(cig, match_words)  # find matches
  names(matches) <- cig                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)

  nam <- names(matches)                # retreive names of remaining software
  
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)})
  
  # if there is a match
  if(length(temp) > 0){
    # get the software name for each df
    for(j in 1:length(temp)){         
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  # if there is no match
  if(length(temp) == 0){
    df[[i]]$software_name <- NA
    df[[i]]$value         <- NA
    df[[i]]$paper         <- p[i]
  }
}

# combine into one final data frame
df1 <- do.call(rbind.data.frame, df)

# number of papers that don't mention one of the softwares.
nrow(filter(df1, is.na(software_name)))

# names of papers missing a software package
pull(filter(df1, is.na(software_name)), paper)
```



Now extract sentence token *index* at which software mention occurs, and the sentence token *length* per paper. The ratio of: $\frac{token \space index}{ token \space length}$ is an indication of where in the paper the mention occurs. In the future, find last mention of References so we know if a mention occurs in the references. But as a first pass, this is okay. 

```{r}
# initalize list that will store all clean data frames
df2 <- vector("list", length = length(raw)) # testing with one paper

# loop over all PDFs, and grab token index, token length
for(i in 1:length(raw)){
  x = rs[[i]]                          # select PDF text
  token_length = length(rs[[i]])       # total number of tokens in paper
  indices <- lapply(cig, function(y){return(str_which(x, y))})
  names(indices) <- cig                # name list element by software
  indices <- indices[!sapply(indices, identical, integer(0))]  # remove non-matches (integer(0))
  
  nam <- names(indices)                # retreive names of remaining software
  
  temp <- lapply(indices, function(z){ # gather each list into a dataframe
    return(data.frame(token_index = z))})
  
  # if there is a match
  if(length(temp) > 0){
    temp <- bind_rows(temp)              # bind into a dataframe
    temp$token_length <- token_length    # cbind the total token length of the paper
    df2[[i]] <- temp                     # store in final df
  }
  
  # if there is no match
  if(length(temp) == 0){
    temp <- data.frame(token_index = NA, token_length = token_length)
    df2[[i]] <- temp
  }
}

# combine into one final data frame
df2 <- do.call(rbind.data.frame, df2)

# combine with previous dataframe of mention windows, software namaes, paper names
final <- cbind.data.frame(df1, df2)

# calculate (token_index / token_length) = mention position in paper
final <- mutate(final, token_position = token_index / token_length)
```


Determine if a mention occurs in the References section. Not straightforward to simply extract "references" because section headers are often not 
```{r}
# rsl <- lapply(rs, tolower) # lowercase
# temp <- lapply(rsl, str_which, "references")
# temp <- lapply(rs, str_which, "References")
```


Sanity check. Also, calculate total mentions and total unique mentions.
```{r}
# total mentions
total_mentions <- final %>% 
  filter(!is.na(software_name) & software_name != "Rayleigh") %>% 
  nrow()

# unique mentions per paper
unique_mentions <- final %>% 
  group_by(software_name) %>% 
  summarise(n_papers = n_distinct(paper)) %>% 
  filter(!is.na(software_name)) %>% 
  arrange(desc(n_papers)) %>% 
  filter(software_name != "Rayleigh") %>% 
  pull(n_papers) %>% 
  sum()

# Lorraine was getting 279 total mentions, and I get 243. This automated pipeline works.
```



Visualize distribution of position occurence.
```{r}
# library(ggridges)
# final %>% 
#   filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ") ) %>% 
#   ggplot(aes(token_position, software_name, fill = ..x..)) + 
#   geom_density_ridges_gradient(scale = 1.5, rel_min_height = 0.01) + 
#   scale_fill_viridis(option = "magma")

med_vec <- final %>% 
  # filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ", "Rayleigh") ) %>%
  filter(!is.na(software_name) & !software_name %in% c("Rayleigh") ) %>%
  group_by(software_name) %>% 
  summarise(med = median(token_position)) %>% 
  arrange(-med) %>% pull(software_name)

# define color pallette
cols <- colormap(colormap = colormaps$viridis, nshades = length(med_vec))
cols <- structure(cols, names = med_vec)

 
p <- final %>% 
  # filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ", "Rayleigh") ) %>%
  filter(!is.na(software_name) & !software_name %in% c("Rayleigh") ) %>%
  ggplot(aes(x=reorder(software_name,token_position, FUN = median), token_position, fill = software_name)) + 
  geom_boxplot(alpha = 0.5) + 
  geom_jitter(width = 0.2, alpha = 0.1) + 
  coord_flip() + 
  theme_minimal(base_size = 15) + 
  guides(fill = FALSE) +
  scale_fill_manual(values = cols, 
                    breaks = med_vec,
                    name = "Well Type") +
  labs(title = "CIG software mentions (2010-2015)", 
       subtitle = glue("Total mentions (n = {total_mentions}); Unique mentions (n = {unique_mentions})"),
       x = "", y = "Location of Mention in Document", 
       # caption = "SW4, SNAC, MAG, Rayleigh removed")
       caption = "Rayleigh removed")
p

# save
ggsave(p, filename = "cig_total_and_unique_mentions_all.pdf", device = cairo_pdf, width = 11, height = 8)
# ggsave(p, filename = "cig_total_and_unique_mentions_trunc.pdf", device = cairo_pdf, width = 11, height = 8)

getwd()
```


Make adjacency matrix of software packages, and distance matrix. Make into graph.
```{r}

```

View the co-occurence network.
```{r}

```


Clever way of determining if something is a reference or not. Can be rule-based. Look at mention window. Do the adjacent entries contain "referencey" items like "A."? Replot `p`, but make the references mentions another color.
```{r}

```

























```{r}
myfun <- function(y){
  
  # intialize return variable
  hold <- NA
  
  # get index of all word matches
  i <- str_which(x, y)  

  return(hold) # original working
  return(data.frame(sen = hold, i = i)) # adding sentence index to hold vector
}

df <- list()

for(i in 1:1){
  x = rs[[i]]                          # select PDF text
  matches <- lapply(cig, myfun)        # find matches
  names(matches) <- cig                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)
  
  nam <- names(matches)                # retreive names of remaining software
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)}) 
  
  if(length(temp) > 0){
    for(j in 1:length(matches)){         # fill in the software name for each df
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  if(length(temp) == 0){
    df[[i]] <- NA
  }
}



```



Combine with Duncan's ReadPDF
```{r}
# first paper's names(ReadPDF::getSectionText(p[1])), where p[1] is XML
zn <- c("", "Introduction", "", "Numerical Model of Fault Slip", "", 
"Finite-Element Mesh Processing", "", "Solver Customization", 
"", "Performance Benchmark", "", "Code Verification Benchmarks", 
"", "Conclusions", "", "Notation", "References", "<other>", "Table1", 
"Table2", "Table3", "Table4", "Table5", "Table6", "Table7", "Table8"
)

# remove artifacts from ReadPDF
zn <- zn[! zn %in% c("", "<other>") ] 
z <- zn[-str_which(zn, "Table")]      # vector of terms to search


# look for indices of z in pdf_text of p[1]
raw[[1]] # p[1]
rs[[1]]  # p[1]

```



Repeat with lowercase.
```{r}
rsl  <- lapply(rs, tolower)
cigl <- tolower(cig)


# intalize list that will store all clean data frames
df <- vector("list", length = length(p)) 

# loop over all PDFs, and grab mention windows
library(tidyr)
for(i in 1:length(p)){
  x = rsl[[i]]                          # select PDF text
  matches <- lapply(cigl, match_words)  # find matches
  names(matches) <- cigl                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)
  
  nam <- names(matches)                # retreive names of remaining software
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)}) 
  
  if(length(temp) > 0){
    for(j in 1:length(matches)){         # fill in the software name for each df
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  if(length(temp) == 0){
    df[[i]] <- NA
  }
}

# combine into one final data frame
final <- do.call(rbind.data.frame, df)
```


```{r}
final %>% 
  group_by(software_name) %>% 
  summarise(n_papers = n_distinct(paper)) %>% 
  filter(!is.na(software_name)) %>% 
  arrange(desc(n_papers)) %>% 
  filter(software_name != "rayleigh") %>% 
  pull(n_papers) %>% 
  sum()
```


