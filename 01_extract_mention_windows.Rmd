---
title: "Extract Mention Windows"
output: html_document
---


The purpose of this script is to read in all PDFs from 2010-2015 and extract the senteces matching CIG software, with a window of one sentence before and after the software mention.

Packages.
```{r}
library(pdftools)
library(tokenizers)
library(stringr)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(viridis)
library(colormap)
```


Vector of package names.
```{r}
cig <- c("ASPECT", "Aspect",
"AxiSEM", "axisem", "AXISEM",
"Burnman", "burnman",
"Calypso",
"Citcom", "citcomcu", "citcomS", "citcoms", "CitcomS", "CITCOM-CU",
"ConMan", "conman",
"Ellipsis3d", "ellipsis", "ellipsis3d", "Ellipsis",
"FLEXWIN", "flexwin",
" Gale ",    # avoid "Galerkin"
" MAG ",     # avoid picking up "magnetic", "magnitude", etc..
"Mineos", "mineos", "MINEOS",
"PyLith", "pylith",
# "Rayleigh", # too many false positives. will need to do manually later
"RELAX",
"SEISMIC_CPML", "seismic cpml", 
"SELEN", "selen ",
"SNAC",
"SPECFEM", "specfem2d", "specfem3d_globe", "specfem3d_cartesian", "Specfem3-D Globe",
"SW4", "sw4",
"Virtual California",
"Gmsh", "wpp", "GMT")

#write_tsv(x = data.frame(term = cig), path = "C:/Users/rpauloo/Desktop/whitelist.tsv")
```


```{r}
fp <- "C:/Users/rpauloo/Desktop/2019 CItation/Papers/" # PC
# fp <- "/Users/richpauloo/Desktop/2019 CItation/Papers/" # Mac

# paper names
p <- list.files(paste0(fp, "all_papers")) 
```

For one PDF, get vector of paper file names, read in the first one, tokenize sentences, and extract the mention window.
```{r}
# # pdf_info(paste0(fp, "all_papers/", p[1])) # metadata: can grab all DOI
# 
# # read all pdfs
# raw <- paste(pdf_text(paste0(fp, "all_papers/", p[1])), collapse = " ")
#   
# # tokensize sentence with tokenizers package
# rs <- tokenize_sentences(raw[1])                      # tokenize sentences
# rs <- lapply(rs, function(x){paste(x, collapse = " ")}) # collapse each list element
# 
# # do for one
# x = rs[[1]]
# 
# # grabs mention window if a software (y) is mentioned
# match_words <- function(y){
#   hold <- NA
#   i    <- str_which(x, y)
#   if(!identical(i, integer(0))){
#     hold <- paste(x[i-2], x[i-1], x[i], x[i+1], x[i+2])
#   }
#   # hold <- data.frame(match = y, matches = hold)
#   return(hold)
# }
# 
# # applies vector of software names to `match_words` to get list of all windows
# matches <- lapply(cig, match_words)
# 
# # name the list by software name, filter out NA, and organize into a dataframe
# names(matches) <- cig
# matches <- matches[!is.na(matches)] 
# data.frame(matches) %>% 
#   gather(software_name, value) %>% 
#   distinct()
```

Function to extract mention windows to all PDFs.
```{r}
# function to read PDFs
read_pdfs <- function(p){
  paste(pdf_text(paste0(fp, "all_papers/", p)), collapse = " ")
}

# read all pdfs into a list: takes a while
# raw <- lapply(p, read_pdfs)
# readr::write_rds(raw, "raw.rds") # save the data
raw <- read_rds("raw.rds")

# tokenize sentences
rs <- tokenize_sentences(raw)

# find mention windows for all words in all PDFs

# function to match words and return mention windows within one PDF
match_words <- function(y){
  
  # intialize return variable
  hold <- NA
  
  # get index of all word matches
  i    <- str_which(x, y)  
  
  # if there's a match
  if(!identical(i, integer(0))){ 
    # case 1: match at first sentence
    if(1 %in% i){
      hold <- paste(x[i], x[i+1], x[i+2])
    }
    # case 2: match at second sentence
    else if(2 %in% i){
      hold <- paste(x[i-1], x[i], x[i+1], x[i+2])
    }
    # case 3: match at last sentence
    else if(length(x) %in% i){
      hold <- paste(x[i-2], x[i-1], x[i])
    }
    # case 4: match at second to last sentence
    else if((length(x) - 1) %in% i){
      hold <- paste(x[i-2], x[i-1], x[i], x[i+1])
    }
    # case 5: match NOT at 1st, 2nd, 2nd to last, or last sentence
    else{
      hold <- paste(x[i-2], x[i-1], x[i], x[i+1], x[i+2])
    }
  }
  return(hold) # original working
}
```

Extract mention windows, and record: software name match, paper title. Store in a data frame.
```{r}
# initalize list that will store all clean data frames
df <- vector("list", length = length(raw)) # testing with one paper

# loop over all PDFs, and grab mention windows
for(i in 1:length(raw)){
  x = rs[[i]]                          # select PDF text
  matches <- lapply(cig, match_words)  # find matches
  names(matches) <- cig                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)

  nam <- names(matches)                # retreive names of remaining software
  
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)})
  
  # if there is a match
  if(length(temp) > 0){
    # get the software name for each df
    for(j in 1:length(temp)){         
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  # if there is no match
  if(length(temp) == 0){
    df[[i]]$software_name <- NA
    df[[i]]$value         <- NA
    df[[i]]$paper         <- p[i]
  }
}

# combine into one final data frame
df1 <- do.call(rbind.data.frame, df)

# recombine into main group names
df1 <- mutate(df1, software_name = case_when(software_name %in% c("axisem", "AXISEM") ~ "AxiSEM",
                                             software_name == "burnman" ~ "Burnman",
                                             software_name %in% c("citcomcu", "citcomS", "citcoms", "CitcomS", "CITCOM-CU") ~ "Citcom",
                                             software_name == "conman" ~ "ConMan",
                                             software_name %in% c("ellipsis", "ellipsis3d", "Ellipsis") ~ "Ellipsis3d",
                                             software_name %in% c("mineos", "MINEOS") ~ "Mineos",
                                             software_name == "flexwin" ~ "FLEXWIN",
                                             software_name == "pylith" ~ "PyLith",
                                             software_name == "seismic cpml" ~ "SEISMIC_CPML",
                                             software_name %in% c("specfem2d", "specfem3d_globe", "specfem3d_cartesian", "Specfem3-D Globe") ~ "SPECFEM",
                                             software_name == "sw4" ~ "SW4",
                                             software_name == "selen " ~ "SELEN",
                                             software_name == "Aspect" ~ "ASPECT",
                                             TRUE ~ software_name))
# count of unique mentions in all docs
df1 %>% count(software_name) %>% arrange(desc(n)) 

# count of mentions per unique papers
df1 %>% count(software_name, paper) %>% arrange(desc(n)) 

# number of papers that don't mention one of the softwares.
nrow(filter(df1, is.na(software_name)))

# names of papers missing a software package
pull(filter(df1, is.na(software_name)), paper)
```

Determine co-mentions.
```{r}
n_co_mentions <- df1 %>% group_by(paper) %>% summarise(n_co_mentions = length(unique(software_name)))

# grab co-mentions per paper
l <- split(df1, df1$paper)
l <- sapply(l, function(x){paste(unique(x$software_name), collapse = ", ")})
l <- data.frame(paper = names(l), co_mentioned_software = l)
rownames(l) <- NULL

# merge
df2 <- left_join(n_co_mentions, l, by = "paper") %>% filter(n_co_mentions >= 2) 
```


Write data.
```{r}
library(xlsx)
write.xlsx(df1, "C:/Users/rpauloo/Documents/GitHub/cig_nlp/results/mention_windows.xlsx", row.names = FALSE)
write.xlsx(data.frame(paper = pull(filter(df1, is.na(software_name)), paper)), 
          "C:/Users/rpauloo/Documents/GitHub/cig_nlp/results/missing_software.xlsx", row.names = FALSE)
write.xlsx(as.data.frame(df2), "C:/Users/rpauloo/Documents/GitHub/cig_nlp/results/co_mention_summary.xlsx", row.names = FALSE)
```


Now extract sentence token *index* at which software mention occurs, and the sentence token *length* per paper. The ratio of: $\frac{token \space index}{ token \space length}$ is an indication of where in the paper the mention occurs. In the future, find last mention of References so we know if a mention occurs in the references. But as a first pass, this is okay. 

```{r}
# initalize list that will store all clean data frames
df3 <- vector("list", length = length(raw)) # testing with one paper

# loop over all PDFs, and grab token index, token length
for(i in 1:length(raw)){
  x = rs[[i]]                          # select PDF text
  token_length = length(rs[[i]])       # total number of tokens in paper
  indices <- lapply(cig, function(y){return(str_which(x, y))})
  names(indices) <- cig                # name list element by software
  indices <- indices[!sapply(indices, identical, integer(0))]  # remove non-matches (integer(0))
  
  nam <- names(indices)                # retreive names of remaining software
  
  temp <- lapply(indices, function(z){ # gather each list into a dataframe
    return(data.frame(token_index = z))})
  
  # if there is a match
  if(length(temp) > 0){
    temp <- bind_rows(temp)              # bind into a dataframe
    temp$token_length <- token_length    # cbind the total token length of the paper
    df3[[i]] <- temp                     # store in final df
  }
  
  # if there is no match
  if(length(temp) == 0){
    temp <- data.frame(token_index = NA, token_length = token_length)
    df3[[i]] <- temp
  }
}

# combine into one final data frame
df3 <- do.call(rbind.data.frame, df3)

# combine with previous dataframe of mention windows, software namaes, paper names
final <- cbind.data.frame(df1, df3)

# calculate (token_index / token_length) = mention position in paper
final <- mutate(final, token_position = token_index / token_length)
```


Determine if a mention occurs in the References section. Not straightforward to simply extract "references" because section headers are often not 
```{r}
# rsl <- lapply(rs, tolower) # lowercase
# temp <- lapply(rsl, str_which, "references")
# temp <- lapply(rs, str_which, "References")
```


Sanity check. Also, calculate total mentions and total unique mentions.
```{r}
# total mentions
total_mentions <- final %>% 
  filter(!is.na(software_name) & software_name != "Rayleigh") %>% 
  nrow()

# unique mentions per paper
unique_mentions <- final %>% 
  group_by(software_name) %>% 
  summarise(n_papers = n_distinct(paper)) %>% 
  filter(!is.na(software_name)) %>% 
  arrange(desc(n_papers)) %>% 
  filter(software_name != "Rayleigh") %>% 
  pull(n_papers) %>% 
  sum()

# Lorraine was getting 279 total mentions, and I get 243. This automated pipeline works.
```



Visualize distribution of position occurence.
```{r}
# library(ggridges)
# final %>% 
#   filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ") ) %>% 
#   ggplot(aes(token_position, software_name, fill = ..x..)) + 
#   geom_density_ridges_gradient(scale = 1.5, rel_min_height = 0.01) + 
#   scale_fill_viridis(option = "magma")

med_vec <- final %>% 
  # filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ", "Rayleigh") ) %>%
  filter(!is.na(software_name) & !software_name %in% c("Rayleigh") ) %>%
  group_by(software_name) %>% 
  summarise(med = median(token_position)) %>% 
  arrange(-med) %>% pull(software_name)

# define color pallette
cols <- colormap(colormap = colormaps$viridis, nshades = length(med_vec))
cols <- structure(cols, names = med_vec)

library(glue)
p <- final %>% 
  # filter(!is.na(software_name) & !software_name %in% c("SW4","SNAC"," MAG ", "Rayleigh") ) %>%
  filter(!is.na(software_name) & !software_name %in% c("Rayleigh") ) %>%
  ggplot(aes(x=reorder(software_name,token_position, FUN = median), token_position, fill = software_name)) + 
  geom_boxplot(alpha = 0.5) + 
  geom_jitter(width = 0.2, alpha = 0.1) + 
  coord_flip() + 
  theme_minimal(base_size = 15) + 
  guides(fill = FALSE) +
  scale_fill_manual(values = cols, 
                    breaks = med_vec,
                    name = "Well Type") +
  labs(title = "CIG software mentions (2010-2015)", 
       subtitle = glue("Total mentions (n = {total_mentions}); Unique mentions (n = {unique_mentions})"),
       x = "", y = "Location of Mention in Document", 
       # caption = "SW4, SNAC, MAG, Rayleigh removed")
       caption = "Rayleigh removed")
p

# save
#ggsave(p, filename = "cig_total_and_unique_mentions_all.pdf", device = cairo_pdf, width = 11, height = 8)
# ggsave(p, filename = "cig_total_and_unique_mentions_trunc.pdf", device = cairo_pdf, width = 11, height = 8)

#getwd()
```

Make Lorraine's graph.
```{r}
sw <- unique(final$software_name)
sw <- sw[!is.na(sw)]
p_list <- vector("list", length = length(sw))

for(i in 1:length(p_list)){
  p_list[[i]] <- final %>% 
  filter(software_name == sw[i]) %>% 
  ggplot(aes(factor(paper), token_position, color = token_position)) +
  geom_point(size = 2) +
  coord_flip() + 
  scale_color_viridis_c() +
  theme(panel.background = element_rect(fill = "white"),
        panel.grid.major.y = element_line(color = "grey50"),
        panel.grid.major.x = element_blank()) +
  guides(color = FALSE) +
  labs(x = "", y = "Position in text", 
       title = sw[i])
}

# Creating new PDF file
pdf("software_mentions.pdf", width = 11 , height = 8)
 
for(i in 1:length(p_list)){
   print(p_list[[i]])
}
 
dev.off()
```

Turn co-occurences into plot.
```{r}
co <- df2 %>% select(-paper) %>% rename(n = n_co_mentions, x = co_mentioned_software)

library(tidyverse)
library(combinat)
cor <- co %>% 
    ## Parse entries in x into distinct elements
    mutate(split = map(x, str_split, pattern = ', '), 
           flat = flatten(split)) %>% 
    ## Construct 2-element subsets of each set of elements
    mutate(combn = map(flat, combn, 2, simplify = FALSE)) %>% 
    unnest(combn) %>% 
    ## Construct permutations of the 2-element subsets
    mutate(perm = map(combn, permn)) %>% 
    unnest(perm) %>% 
    ## Parse the permutations into row and column indices
    mutate(row = map_chr(perm, 1), 
           col = map_chr(perm, 2)) %>% 
    count(row, col) %>% 
    ## Long to wide representation
    spread(key = col, value = nn, fill = 0) %>% 
    ## Coerce to matrix
    column_to_rownames(var = 'row') %>% 
    as.matrix()

library(reshape2)
cor[upper.tri(cor, diag=TRUE)] <- NA # remove upper triangle
melt(cor) %>% ggplot(aes(Var1, Var2, fill = value)) + geom_raster()

co_plot <- melt(cor) %>% 
  filter(!is.na(value)) %>% 
  ggplot(aes(Var1, Var2, fill = value)) + 
  geom_tile(color = "white") + 
  geom_text(aes(label = value), color = "grey50") +
  scale_fill_viridis_c() +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(fill = "Co-occurences", x = "", y = "")

ggsave(co_plot, filename = "co_occurrence_plot.pdf", device = cairo_pdf, height = 5, width = 11)
```


Make adjacency matrix of software packages, and distance matrix. Make into graph.
```{r}

```


Clever way of determining if something is a reference or not. Can be rule-based. Look at mention window. Do the adjacent entries contain "referencey" items like "A."? Replot `p`, but make the references mentions another color.
```{r}

```

























```{r}
myfun <- function(y){
  
  # intialize return variable
  hold <- NA
  
  # get index of all word matches
  i <- str_which(x, y)  

  return(hold) # original working
  return(data.frame(sen = hold, i = i)) # adding sentence index to hold vector
}

df <- list()

for(i in 1:1){
  x = rs[[i]]                          # select PDF text
  matches <- lapply(cig, myfun)        # find matches
  names(matches) <- cig                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)
  
  nam <- names(matches)                # retreive names of remaining software
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)}) 
  
  if(length(temp) > 0){
    for(j in 1:length(matches)){         # fill in the software name for each df
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  if(length(temp) == 0){
    df[[i]] <- NA
  }
}



```



Combine with Duncan's ReadPDF
```{r}
# first paper's names(ReadPDF::getSectionText(p[1])), where p[1] is XML
zn <- c("", "Introduction", "", "Numerical Model of Fault Slip", "", 
"Finite-Element Mesh Processing", "", "Solver Customization", 
"", "Performance Benchmark", "", "Code Verification Benchmarks", 
"", "Conclusions", "", "Notation", "References", "<other>", "Table1", 
"Table2", "Table3", "Table4", "Table5", "Table6", "Table7", "Table8"
)

# remove artifacts from ReadPDF
zn <- zn[! zn %in% c("", "<other>") ] 
z <- zn[-str_which(zn, "Table")]      # vector of terms to search


# look for indices of z in pdf_text of p[1]
raw[[1]] # p[1]
rs[[1]]  # p[1]

```



Repeat with lowercase.
```{r}
rsl  <- lapply(rs, tolower)
cigl <- tolower(cig)


# intalize list that will store all clean data frames
df <- vector("list", length = length(p)) 

# loop over all PDFs, and grab mention windows
library(tidyr)
for(i in 1:length(p)){
  x = rsl[[i]]                          # select PDF text
  matches <- lapply(cigl, match_words)  # find matches
  names(matches) <- cigl                # name list element by software
  matches <- matches[!is.na(matches)]  # remove non-matches (NA)
  
  nam <- names(matches)                # retreive names of remaining software
  temp <- lapply(matches, function(z){ # gather each list into a dataframe
    gather(data.frame(z), software_name, value)}) 
  
  if(length(temp) > 0){
    for(j in 1:length(matches)){         # fill in the software name for each df
      temp[[j]]$software_name <- nam[j]
    }
    temp <- do.call(rbind.data.frame, temp) # make into one df
    temp$paper <- p[i]                      # add file name of paper
    rownames(temp) <- NULL                  # remove rownames
    df[[i]] <- temp                         # store in main list
  }
  
  if(length(temp) == 0){
    df[[i]] <- NA
  }
}

# combine into one final data frame
final <- do.call(rbind.data.frame, df)
```


```{r}
final %>% 
  group_by(software_name) %>% 
  summarise(n_papers = n_distinct(paper)) %>% 
  filter(!is.na(software_name)) %>% 
  arrange(desc(n_papers)) %>% 
  filter(software_name != "rayleigh") %>% 
  pull(n_papers) %>% 
  sum()
```


